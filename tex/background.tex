%!TEX root = ClementiBarba2020.tex

\subsection{Verification, validation, reproducibility and replication}

Verification and validation of computational models---often abbreviated V\&V and viewed in concert---have developed into a mature subject with more than two decades of organized efforts to standardize it, dedicated conferences, and a  journal. 
The American Society of Mechanical Engineers (ASME), a standards-developing organization, formed its first Verification and Validation committee (known as V\&V 10) in 2001, with the charter: 
``to develop standards for assessing the correctness and credibility of modeling and simulation in computational solid mechanics.''
It approved its first document in 2006: The Guide for Verification and Validation in Computational Solid Mechanics (known as V\&V 10-2006). 
The fact that this guide was five years in the making illustrates just how complex the subject matter, and building consensus about it, can be. 
Since that first effort, six additional standards sub-committees have tackled V\&V in a variety of contexts. 
V\&V 70 is the latest, focused on machine-learning models.
The key principles laid out in the first V\&V standard persevere through the many subsequent efforts that have operated to this day. 
They are:

\begin{compactitem}

\item[$\triangleright$] Verification must precede validation.
\item[$\triangleright$] The need for validation experiments and the associated accuracy requirements for computational model predictions are based on the intended use of the model.
\item[$\triangleright$] Validation of a complex system should be pursued in a hierarchical fashion from the component level to the system level.
\item[$\triangleright$] Validation is specific to a particular computational model for a particular intended use.
\item[$\triangleright$] Validation must assess the predictive capability of the model in the physical realm of interest, and it
must address uncertainties that arise from both simulation results and experimental data.

\end{compactitem}

\noindent
The process of \emph{verification} establishes that a computational model correctly describes the intended mathematical equations and their solutions.
It encompasses both code correctness, and solution accuracy.
\emph{Validation}, on the other hand, seeks to determine to which measure a computational model represents the physical world. 
We like to say that ``verification is solving the equations right, and validation is solving the right equations'' \cite{roache1998}. 
But in reality the exercise can be much more complicated than this sounds. 
Computational models in most cases are built in a hierarchy of simplifications and approximations, and comparing with the physical world means conducting experiments, which themselves carry uncertainties. 

As we will discuss in this paper, verification and validation in contexts that involve complex physics at less tractable scales (either very small, or very large), or where experimental methods are nascent, proceeds in a tangle of researcher judgements and path finding. 
In practice, validation activities reported in the scholarly literature often concentrate on using a stylized benchmark, and comparing experimental measurements with the results from computational models on that benchmark. 
Seldom do these activities address the key principles of pursuing validation in a hierarchical fashion from the component to the system level, and of assessing the predictive capability of the computational model accounting for various sources of uncertainties. 
Comprehensive validation studies are difficult, expensive, and time consuming. 
Often, they are severely limited by practical constraints, and the conclusions equivocal. 
Yet the computational models still provide useful insights into the research or engineering question at hand, and we build trust on them little by little.

Verification and validation align on one axis of the multi-dimensional question of when are claims to knowledge arising from modeling and simulation justified, credible, true \cite{winsberg-2010}.
Two other axes of this question are: reproducibility and replication, and uncertainty quantification (UQ).
Uncertainty quantification uses statistical methods to give objective confidence levels for the results of simulations. 
Uncertainties typically stem from input data, modeling errors, genuine physical uncertainties, random processes, and so on. 
A scientific study may be reproducible, the simulations within it undergone V\&V, yet the results are still uncertain. 
Building confidence in scientific findings obtained through computational modeling and simulation entails efforts in the three ``axes of truth'' described here.

Reproducibility and replication (we could call it R\&R) preoccupy scientific communities more recently. 
Agreement on the terminology, to begin with, has been elusive \cite{barba2018}. 
The National Academies of Science, Engineering and Medicine (NASEM) released in May 2019 a consensus study report on Replicability and Reproducibility in Science \cite{NASEM2019} with definitions as follows.
``Reproducibility is obtaining consistent results using the same input data, computational steps, methods, and code, and conditions of analysis.
Replicability is obtaining consistent measurements or results, or drawing consistent conclusions using new data, methods, or conditions, in a study aimed at the same scientific question.''
According to these definitions, reproducibility demands full transparency of the computational workflow, which at the very least means open code and open data, where `open` means shared at time of publication (or earlier) under a standard public license. 
This condition is infrequently satisfied.
The NASEM report describes how a number of systematic efforts to reproduce computational results have failed in more than half of the attempts made, mainly due to inadequately specified or unavailable data, code and computational workflow \cite{moraila-etal-2014,iqbal-etal-2016,chang-li2018,stodden-etal-2018}. 
Recommendation 4-1 of the NASEM report states that 
``to help ensure reproducibility of computational results, researchers should convey clear, specific, and complete information about any computational methods and data products that support their published results in order to enable other researchers to repeat the analysis, unless such information is restricted by nonpublic data policies. That information should include the data, study methods, and computational environment'' \cite{NASEM2019}.

Although it may seem evident that running an analysis with identical inputs would result in identical outputs, this is sometimes not true. 
For example, the combination of floating-point representation of numbers and parallel processing means that running the same software with the same input data may give different numerical results. 
In some research settings, it may make sense to relax the requirement of bitwise reproducibility and settle on reproducible results within an accepted range of variation (or uncertainty). 
This can only be decided, however, after fully understanding the numerical-analysis issues affecting the outcomes---and the risk associated with an uncertainty range. 
Researchers using high-performance computing thus recognize that when different runs with the same input data produce slightly different numeric outputs, each of these results is equally credible, and the output must be understood as an approximation to the correct value within a certain accepted uncertainty.

Beyond the particulars of high-performance or parallel computing, 
a number of factors can contribute to the lack of reproducibility in research. 
In addition to lack of access to non-public data and code, the NASEM report (of which Barba is a co-author) lists the following contributors to lack of reproducibility:

\begin{compactitem}

\item[$\triangleright$] Inadequate record-keeping: the researchers did not properly document all relevant digital artifacts and steps followed to obtain the results, the details of the computational environment, software dependencies, and/or identifiers and metadata for data products.
\item[$\triangleright$]  Lack of transparency: the researchers did not transparently report, using standard public licenses, an archive with all relevant digital artifacts necessary to reproduce the results.
\item[$\triangleright$]  Obsolescence of the digital artifacts: over time, the digital artifacts in the research compendium are compromised because of technological breakdown and evolution or lack of continued curation.
\item[$\triangleright$]  Flawed attempts to reproduce others' research: the researchers who attempted to reproduce the work lacked expertise or failed to correctly follow the research protocols.
\item[$\triangleright$]  Barriers in the culture of research: lack of resources and incentives to adopt reproducible and transparent research across fields and researchers.

\end{compactitem}

\noindent
Improving computational reproducibility hinges on capturing and sharing information about the computational environment and steps required to collect, process, and analyze data.
Scientific workflows represent a complex flow of data products through various steps of collection, transformation, and analysis to produce an interpretable result. 
Capturing provenance of the result is increasingly difficult to do using manual processes, and automation is key. 
With regards to software management, version-control systems are used to automatically capture the history of all changes made to the source code of a computer program. 
This creates a history of changes and allows the developers to better understand the code and to identify possible problems or errors.
Recent technological advances in version control, virtualization, computational notebooks, and automatic provenance tracking have the potential to simplify reproducibility, and tools have been developed that leverage these technologies.
Still, many questions remain unanswered both to understand the gaps left by existing tools and to develop principled approaches that fill those gaps. 

Replication of scientific findings is key for building trust in them. 
It is often difficult to attain, for many reasons, not least because deciding when two scientific findings are \emph{consistent} is tangled in researcher judgements and inevitable constraints. 
The NASEM report lists the following factors affecting the replicability of findings: 

\begin{compactitem}

\item[$\triangleright$] the complexity of the system under study;
\item[$\triangleright$] understanding of the number and relations among variables within the system under study;
\item[$\triangleright$] the ability to control the variables; 
\item[$\triangleright$] levels of noise within the system (or signal to noise ratios);
\item[$\triangleright$] the mismatch of scale of the phenomena and the scale at which it can be measured; 
\item[$\triangleright$] stability across time and space of the underlying principles; 
\item[$\triangleright$] fidelity of the available measures to the underlying system under study (e.g., direct  or indirect measurements); and
\item[$\triangleright$] prior probability (pre-experimental plausibility) of the scientific hypothesis.

\end{compactitem}

\noindent
Fields of study that have been at the center of the `replication crisis` commotion tend to be characterized by their complexity, intrinsic variability, or inability to control variables, e.g., psychology. 
But in many areas of modern technology we face similar challenges to control variables or disentangle many interacting effects. 
In this paper, we tackle replication and validation of computational models in nanoscale physics, where certainly the systems are complex, variables difficult to control, and signals subject to noise.

\subsection{Description of the PyGBe software}

Our research group has been developing PyGBe---pronounced \emph{pigb\={e}}---for several years. 
It was first written for biomolecular-electrostatics calculations using a continuum model of proteins in water or an ionic solvent. 
The computational model applies a boundary integral form of the governing Poisson-Boltzmann and Poisson equations, to obtain the electrostatic potential and its normal derivative on the molecular surface. 
In the implicit-solvent model, this information is used to compute the quantity of interest: solvation energy, which is diagnostic for questions of protein binding affinity, protein-surface interactions, and others. 
Biomolecules are modeled as dielectric cavities inside an infinite continuum solvent, and the partial differential equations are solved via a boundary element method, leading to a dense linear system of equations. 
PyGBe uses a fast-summation algorithm via a Barnes-Hut treecode \cite{BarnesHut1986}, and accelerates the computationally intensive parts of the code on Nvidia GPU hardware using CUDA kernels in the treecode, interfacing with PyCUDA \cite{klockner2012pycuda}. 
Other portions of the code are written in C$++$, wrapped using \texttt{swig}, to extract more performance \cite{CooperETal2016}. 
These features allow PyGBe to handle problems with half a million boundary elements or more. 

In more recent work, we expanded the capabilities of PyGBE to applications of computational nanoplasmonics for biosensing \cite{ClementiETal2019}. 
Applying the long-wavelength limit, one can model the phenomenon of localized surface plasmon resonance (LSPR) via electrostatics, instead of the full Maxwell equations. 
This phenomenon is harnessed in nanosensors for detecting with high sensitivity the presence of biomolecules through shifts in resonance frequency. 
In LSPR, an electromagnetic wave excites the free electrons on the surface of a metallic nanoparticle. 
The name given to these electron-cloud vibrations is plasmons. 
In this setting, they resonate with the incoming field, and the energy is either absorbed by the nanoparticle or scattered in all directions, causing what's called extinction.
Since the resonance frequency depends on the dielectric environment, the change produced by a biomolecule approaching the metallic nanoparticle results in a frequency shift, and a means of detecting its presence.
The electrostatic approximation allows using the methods implemented in PyGBe, after substantial code development to permit using complex-valued permittivities, and to incorporate an external electric field into the model. 
These changes included re-writing the Krylov iterative solver to work with complex numbers, and splitting the treecode evaluation into real and imaginary parts. 
New functions were added to read from configuration files the data describing the electric field, to compute the dipole moment, and to compute the final quantity of interest: extinction cross-section. 
We reported a major new release of the software in 2017 \cite{ClementiETal2017}, and later  presented the mathematical formulation for electromagnetic scattering in the long-wavelength setting and its associated continuum and discretized integral equations, and results including verification activities and sensitivity calculations on a biosensor model \cite{ClementiETal2019}. 
For verification, we conducted grid-convergence studies in two settings.
In the first, we set up a computational model for a spherical silver nanoparticle in a constant electric field, leading to an analytical solution for the extinction cross-section. 
The second case does not have a closed form solution: a spherical nanoparticle with a nearby protein (analyte), under an electric field. 
To estimate relative errors in the extinction cross-section, in this case, we made use of the method of Richardson extrapolation.
These verification activities build our confidence on the computational model---moreover, our work was published following careful reproducibility practices, including the release of reproducibility packages for all results. 
Nevertheless, the gold standard of confidence in the predictive capability of the computational model comes from validation: our quest for an opportunity to conduct validation studies with PyGBe in these settings led us through the twisted path that we report in this paper.


\subsection{Physics context for this work}

In recent years, polar dielectric crystals such as Silicon Carbide (SiC) became recognized as an alternative to plasmonic metals in many technologies, including biosensors. 
They manifest oscillations of lattice-bound charges, called surface phonon polaritons, in the mid- to long-wave infrared range with low optical losses.
Nanostructures made of these materials offer sensing capabilities, described by their \emph{figure of merits}, that are unattainable with plasmonic metals. 
The figure of merits of a nanoparticle is defined as the ratio between the sensitivity and the width of the resonance peak at mid-height \cite{otte-etal-2012}. 
In turn, sensitivity is the shift in the resonance position divided by the change in the refractive index: 
$S = \Delta \lambda / \Delta n$.
The dielectric function of polar dielectrics has a negative real part and a small imaginary part, in the mid to long infrared regime. 
This dielectric behavior is similar to that observed in metals like silver in the blue part of the wavelength spectrum. 
In plasmonics, when illuminating a small particle made of metallic materials, we will observe that certain wavelengths excite a surface plasmon. 
The main difference with polar dielectrics like SiC is that for this material the frequency of the incoming light matches instead the resonance frequency of the Si and C sub-lattices \cite{caldwell2015,rockstuhl2005}. 
This excitation leads to a strong extinction cross-section at the resonance wavelength, and an enhanced near-field amplitude. 
These behaviors can be modeled with the same approaches used for localized surface plasmon resonance, and when the wavelength is much larger than the size
of the nanoparticle, we can again apply the electrostatic approach implemented in \pygbe \cite{ClementiETal2017, ClementiETal2019}.

When studying both surface phonon polaritons and surface plasmon resonances, one can analyze the spectrums by measuring different quantities. 
We can measure scattering cross-section, extinction cross-section (scattering plus absorption), as well as reflection. 
These different approaches are comparable since the \textit{quantity of interest} is the wavelength (frequency) at which the resonance modes happen. 
Whether we measure reflection or extinction cross-section, the wavelength (frequency) at which the peaks happen remains the same. 
Throughout this work, we will concentrate on studying the wavelength (frequency) at which the resonance modes occur, 
and we aim to replicate results from Rockstuhl et al.\ 2005 \cite{rockstuhl2005} and from Ellis et al.\ 2016 \cite{ellis2016}, 
and to validate our software against experimental results from Ellis et al.\ 2016 \cite{ellis2016}.
